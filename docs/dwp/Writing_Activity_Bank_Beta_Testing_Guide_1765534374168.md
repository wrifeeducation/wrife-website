# WriFe Writing Activity Bank
## BETA TESTING GUIDE

**Complete Testing Protocol for 40-Level Progressive Writing Programme**

---

## BETA TESTING OVERVIEW

### Objectives

**Primary Goals:**
1. Validate AI assessment accuracy across all 40 levels
2. Verify pedagogical effectiveness of progression
3. Test technical infrastructure under real classroom conditions
4. Gather teacher and pupil feedback
5. Identify and fix issues before full launch

**Success Criteria:**
- AI assessment matches teacher judgment 85%+ of time
- Pupils show measurable improvement in writing
- 80%+ pupil engagement and completion
- Technical platform stable (95%+ uptime)
- Teachers report system is usable and valuable

---

## TESTING PHASES

### PHASE 1: Internal Testing (Week 1-2)
**Your Year 4 Class - Controlled Environment**

**Levels to Test:** 1-10 (Tiers 1-2)

**Protocol:**
1. Introduce system to class (10 minutes)
2. Pupils complete Levels 1-3 supervised
3. Observe pupil interactions
4. Review AI feedback quality
5. Interview 5-6 pupils about experience
6. Document issues

**Data Collection:**
- Screenshots of all AI feedback
- Pupil completion times
- Error rates
- Technical issues log
- Pupil quotes/reactions
- Teacher observation notes

**Week 1 Focus:** Levels 1-5 (Tier 1)
**Week 2 Focus:** Levels 6-10 (Tier 2)

---

### PHASE 2: Expanded Testing (Week 3-4)
**Add 2-3 Additional Teachers**

**Levels to Test:** 11-24 (Tiers 3-4)

**Protocol:**
1. Teacher training session (30 minutes)
2. Each class completes 2-3 levels per week
3. Teachers complete weekly feedback form
4. Pupils complete end-of-week survey
5. Weekly review meeting with teachers

**Recruitment:**
- Colleagues in your school
- Teachers you trust from network
- Mix of year groups (Year 3, 4, 5)
- Variety of pupil abilities

**Week 3 Focus:** Levels 11-17 (Tier 3 - Simple Sentences)
**Week 4 Focus:** Levels 18-24 (Tier 4 - Sentence Expansion)

---

### PHASE 3: Full Pilot (Week 5-8)
**Scale to 10-15 Classes**

**Levels to Test:** 25-40 (Tiers 5-8)

**Protocol:**
1. Full implementation in multiple schools
2. Teachers use independently
3. Automated data collection
4. Fortnightly teacher check-ins
5. End-of-pilot evaluation

**Week 5:** Levels 25-28 (Tier 5 - Sequencing)
**Week 6:** Levels 29-33 (Tier 6 - BME Structure)
**Week 7:** Levels 34-37 (Tier 7 - Enhanced Sentences)
**Week 8:** Levels 38-40 (Tier 8 - Short Narratives)

---

## TESTING MATERIALS

### Teacher Materials

**1. Teacher Quick Start Guide**
```
WRIFE WRITING ACTIVITY BANK - QUICK START

WHAT IS IT?
40 progressive writing activities where pupils write in a text box and receive AI-powered feedback.

FIRST STEPS:
1. Log in to app.wrife.co.uk with teacher account
2. Access your class list
3. Click "Writing Activities"
4. Assign Level 1 to all pupils
5. Show pupils how to access (5 min demo)

PUPIL LOGIN:
- Website: app.wrife.co.uk
- Username: [first name][last initial]
- Password: [provided]

DURING ACTIVITY (5-10 minutes):
- Pupils work independently
- You monitor progress in real-time
- Help with technical issues only
- Don't preview AI feedback

AFTER ACTIVITY:
- Review AI feedback for accuracy
- Flag any concerning assessments
- Check class progress dashboard
- Note any patterns

WEEKLY TASK:
Complete feedback form (5 minutes)

QUESTIONS? Email: michael@wrife.co.uk
```

**2. Weekly Teacher Feedback Form**

```
WRIFE WRITING ACTIVITIES - WEEKLY FEEDBACK

Week of: _______________
Your name: _______________
Class: Year ___ 

LEVELS COMPLETED THIS WEEK:
â˜ Level ___ â˜ Level ___ â˜ Level ___

TECHNICAL PERFORMANCE (1-5 scale):
- Platform stability: â˜1 â˜2 â˜3 â˜4 â˜5
- Load times: â˜1 â˜2 â˜3 â˜4 â˜5  
- Ease of use: â˜1 â˜2 â˜3 â˜4 â˜5

AI FEEDBACK QUALITY (tick all that apply):
â˜ Age-appropriate language
â˜ Accurate assessment
â˜ Specific and helpful
â˜ Encouraging tone
â˜ Appropriate difficulty
â˜ Some inaccuracies (describe below)
â˜ Too harsh/gentle

PUPIL ENGAGEMENT:
- How many pupils engaged well? ___/___
- Any pupils struggling? (names/issues)
- Any pupils excelling? (names/notes)

BEST MOMENT THIS WEEK:

BIGGEST PROBLEM THIS WEEK:

AI FEEDBACK TO REVIEW:
Flag any assessments that seemed wrong:
- Level ___, Pupil: _________, Issue: __________

SUGGESTIONS FOR IMPROVEMENT:

TIME COMMITMENT:
- Minutes per week on WriFe activities: ___
- Minutes per week reviewing dashboard: ___
- Worth the time? â˜Yes â˜No â˜Unsure

WOULD YOU CONTINUE USING THIS? â˜Yes â˜No

Additional comments:
```

**3. End-of-Pilot Teacher Interview**

Key Questions:
- Overall experience rating (1-10)
- Would you use this regularly in your classroom?
- How does it compare to current writing practice?
- What worked best?
- What needs improvement?
- How accurate was AI feedback?
- Did pupils improve over time?
- Technical issues encountered?
- Time commitment acceptable?
- Value for pupils?
- Recommendations for changes?

---

### Pupil Materials

**1. Pupil Quick Start Card** (print and laminate, 1 per pupil)

```
MY WRIFE WRITING ACTIVITIES ðŸ“

HOW TO START:
1. Go to: app.wrife.co.uk
2. Click "Pupil Login"
3. Type your username: _______________
4. Type your password: _______________

WHAT TO DO:
âœ“ Read the writing task carefully
âœ“ Do your best writing in the box
âœ“ Take your time!
âœ“ Click "Submit" when finished
âœ“ Read your feedback
âœ“ Earn your badge! ðŸ†

REMEMBER:
â†’ Use capital letters
â†’ Use full stops
â†’ Check your spelling
â†’ Make it interesting!

NEED HELP?
Ask your teacher!
```

**2. Weekly Pupil Survey** (Google Form, 2 minutes)

```
WRIFE WRITING - HOW WAS IT THIS WEEK?

Your name: _______________

1. Did you enjoy writing activities this week?
   â˜ Loved it! ðŸ˜
   â˜ It was good ðŸ˜Š
   â˜ It was okay ðŸ˜
   â˜ Not really ðŸ˜•

2. Was it the right difficulty?
   â˜ Too easy
   â˜ Just right
   â˜ Too hard

3. Did the feedback help you?
   â˜ Yes, a lot!
   â˜ Yes, a bit
   â˜ Not really
   â˜ I didn't understand it

4. What was your favorite part?
   _______________

5. What was tricky?
   _______________

6. Do you want to do more?
   â˜ Yes!
   â˜ Maybe
   â˜ No

7. What would make it better?
   _______________
```

---

## DATA COLLECTION FRAMEWORK

### Quantitative Data

**Platform Metrics (Automated):**
- Completion rates per level
- Average time per level
- Pass rates per level
- Retry rates
- Error patterns frequency
- Badge earning distribution
- Progression velocity
- Device types used
- Browser compatibility
- Load times
- Error rates (technical)

**Assessment Metrics:**
- AI vs teacher agreement rate
- Score distributions per level
- Improvement over time (per pupil)
- Correlation: attempts to mastery
- Error pattern detection accuracy
- Feedback appropriateness ratings

**Engagement Metrics:**
- Daily active users
- Weekly completion rates
- Time on platform
- Voluntary participation rates
- Drop-off points
- Return rates

### Qualitative Data

**Teacher Feedback:**
- Weekly structured feedback forms
- Mid-pilot focus group (Week 4)
- End-of-pilot interviews
- Informal observations shared
- Email correspondence
- Dashboard usage patterns

**Pupil Feedback:**
- Weekly quick surveys
- Small group interviews (sample)
- Observation notes from teachers
- Volunteered comments
- Writing samples comparison (pre/post)

**Parent Feedback (Optional):**
- End-of-pilot questionnaire
- Observations of home practice
- Perceived value

---

## TESTING PROTOCOLS BY LEVEL CATEGORY

### Tier 1-2 Testing (Levels 1-10): Foundation

**Focus Areas:**
- Is task comprehension clear?
- Are instructions at right reading level?
- Is AI feedback encouraging enough?
- Are error patterns detected accurately?
- Do pupils understand what to do?

**Specific Tests:**

**Level 1 (Word Sorting):**
- Can pupils drag/type words correctly?
- Does AI correctly count categories?
- Is feedback appropriate for age?
- Test with intentional errors:
  - All words in one category
  - Random sorting
  - Correct sorting with spelling errors

**Level 3 (Word Pairs):**
- Does AI recognize logical vs illogical pairs?
- How does it handle creative pairings?
- Test edge cases:
  - "Teachers swim" (possible but unlikely)
  - "Birds teach" (impossible)
  - Made-up words

**Level 5 (Four Categories):**
- Tier completion celebration working?
- Next tier unlocked correctly?
- Certificate awarded?

**Validation Method:**
- Teacher reviews ALL Level 1-10 feedback from first 10 pupils
- Flags any inappropriate or inaccurate feedback
- Documents patterns

---

### Tier 3-4 Testing (Levels 11-24): Sentences

**Focus Areas:**
- Grammar assessment accuracy
- Sentence structure detection
- Teaching guidance quality
- Spelling tolerance (age-appropriate)
- Punctuation assessment

**Specific Tests:**

**Level 14 (First Independent Sentences):**
- Is feedback celebratory enough for milestone?
- Does AI recognize sentence fragments?
- How does it handle invented spelling?
- Test with:
  - Perfect sentences
  - Missing capitals/full stops
  - Sentence fragments
  - Run-on sentences

**Level 17 (WHO-WHAT-WHERE):**
- Can AI verify all three elements present?
- Does it recognize incomplete sentences?
- Tier completion trigger working?

**Level 20 ("But" contrasts):**
- Does AI recognize genuine contrasts?
- How does it handle agreement ("and" logic in "but" sentence)?

**Validation Method:**
- Blind comparison: Teacher assesses same 20 samples independently
- Calculate agreement rate (target: 85%+)
- Analyze disagreements for patterns

---

### Tier 5-6 Testing (Levels 25-33): Narrative Structure

**Focus Areas:**
- Story structure assessment
- Coherence evaluation
- Creativity recognition
- BME framework understanding
- Sequencing logic detection

**Specific Tests:**

**Level 29-31 (BME Components):**
- Does AI correctly identify beginning/middle/end content?
- Can it detect when content is in wrong section?
- How does it assess creativity?

**Level 33 (First Complete Story):**
- Is celebration sufficient for major milestone?
- Does AI evaluate story holistically?
- Can it assess narrative flow?
- Test with:
  - Strong story, weak technical
  - Weak story, strong technical
  - Excellent all-around
  - Poor all-around

**Validation Method:**
- Teacher rates stories holistically
- Compare to AI rating
- Check if AI feedback would help pupil improve

---

### Tier 7-8 Testing (Levels 34-40): Advanced Writing

**Focus Areas:**
- Sentence variety recognition
- Literary technique identification
- Sophisticated feedback quality
- Advanced vocabulary assessment
- Complex sentence structure

**Specific Tests:**

**Level 37 (Sentence Variety):**
- Can AI distinguish simple/compound/complex?
- Does it correctly identify subordinate clauses?
- How does it handle ambiguous structures?

**Level 40 (Programme Completion):**
- Is final feedback comprehensive?
- Does it celebrate full journey?
- Is certificate system working?
- Programme completion tracking accurate?

**Validation Method:**
- Expert review of Level 40 assessments (you + experienced colleague)
- Check against published Year 5/6 standards
- Verify pupils have genuinely progressed

---

## ISSUE TRACKING SYSTEM

### Priority Levels

**P1 - CRITICAL (Fix immediately):**
- Platform crashes
- Data loss
- Pupils locked out
- Security issues
- Inappropriate AI feedback (harsh, rude)

**P2 - HIGH (Fix within 24 hours):**
- Incorrect assessment (significantly off)
- Progression blocked incorrectly
- Major UI problems
- Slow load times (>10 seconds)

**P3 - MEDIUM (Fix within week):**
- Minor assessment inaccuracies
- UI inconsistencies
- Unclear instructions
- Small bugs

**P4 - LOW (Consider for future):**
- Feature requests
- Nice-to-have improvements
- Cosmetic issues

### Issue Documentation Template

```
ISSUE REPORT

Issue ID: _______________
Date: _______________
Reported by: _______________

Priority: â˜P1 â˜P2 â˜P3 â˜P4

Category:
â˜ AI Assessment
â˜ Platform/Technical
â˜ UI/UX
â˜ Content/Instructions
â˜ Progression/Unlocking
â˜ Other: _______________

Description:
_______________

Steps to Reproduce:
1. _______________
2. _______________
3. _______________

Expected Behavior:
_______________

Actual Behavior:
_______________

Screenshot/Evidence:
[Attach]

Affected Users:
â˜ All users
â˜ Specific class: _______________
â˜ Specific level: _______________
â˜ Single user: _______________

Resolution:
_______________

Status:
â˜ Open
â˜ In Progress
â˜ Testing
â˜ Resolved
â˜ Closed
```

---

## AI FEEDBACK VALIDATION PROTOCOL

### Sample Size Requirements

**Per Level:**
- Minimum 10 pupil responses
- Mix of abilities (low/medium/high)
- Range of scores (0-100%)

**Review Process:**
1. Pupil completes activity
2. AI generates feedback
3. Teacher reviews pupil work independently
4. Teacher rates AI feedback:
   - Assessment accuracy (1-5)
   - Feedback appropriateness (1-5)
   - Helpfulness (1-5)
5. Teacher notes any concerns
6. Flagged items reviewed by you

### Red Flags for AI Feedback

**Immediate Review Needed:**
- Score drastically different from teacher judgment (>20% off)
- Harsh or discouraging language
- Inappropriate for age level
- Misses obvious errors
- Praises incorrect work
- Teaching point is wrong/confusing
- Technical terminology too advanced

### Calibration Process

**Weekly Calibration Meetings:**
1. Review 5-10 flagged assessments
2. Discuss disagreements
3. Refine rubrics if needed
4. Update system prompt if pattern emerges
5. Document decisions

---

## BETA TESTING SCHEDULE

### Week-by-Week Plan

**WEEK 1: Setup & Level 1-5**
- Monday: System setup, pupil accounts created
- Tuesday: Introduce to class, complete Level 1
- Wednesday: Levels 2-3
- Thursday: Levels 4-5, Tier 1 completion
- Friday: Review week, teacher feedback form

**WEEK 2: Levels 6-10**
- Monday: Levels 6-7
- Tuesday: Levels 8-9
- Wednesday: Level 10, Tier 2 completion
- Thursday: Catch-up for struggling pupils
- Friday: Weekly review, pupil surveys

**WEEK 3: Recruit Teachers + Levels 11-17**
- Monday: Training session for new teachers
- Rest of week: Levels 11-17 (Tier 3)
- Friday: Teacher check-in meeting

**WEEK 4: Levels 18-24**
- Week focus: Tier 4 completion
- Mid-week: Focus group with teachers
- Friday: Review progress, address issues

**WEEK 5-8: Full Pilot Levels 25-40**
- Progressive completion
- Weekly teacher check-ins
- Ongoing data collection
- Issue resolution

**WEEK 9: Evaluation**
- Data analysis
- Teacher interviews
- Pupil focus groups
- Parent feedback (if collected)
- Final report creation

---

## SUCCESS METRICS

### Technical Success

**Platform Performance:**
- â˜‘ 95%+ uptime
- â˜‘ <3 second load times
- â˜‘ <5% error rate
- â˜‘ Compatible across devices
- â˜‘ No data loss incidents

**AI Performance:**
- â˜‘ 85%+ agreement with teachers
- â˜‘ <2% inappropriate feedback
- â˜‘ Error patterns detected accurately
- â˜‘ Progression decisions appropriate

### Pedagogical Success

**Pupil Learning:**
- â˜‘ 70%+ pass rates on first attempt
- â˜‘ Measurable improvement evident
- â˜‘ Fewer errors in later levels
- â˜‘ Technical skills progress
- â˜‘ Writing quality improves

**Engagement:**
- â˜‘ 80%+ completion rates
- â˜‘ 85%+ pupil enjoyment
- â˜‘ 90%+ want to continue
- â˜‘ Voluntary participation
- â˜‘ Time-on-task appropriate

### Teacher Satisfaction

**Usability:**
- â˜‘ Easy to implement
- â˜‘ Minimal teacher time needed
- â˜‘ Dashboard useful
- â˜‘ Feedback valuable
- â˜‘ Would use regularly

**Value:**
- â˜‘ Saves teacher time overall
- â˜‘ Provides insights into pupil writing
- â˜‘ Supports differentiation
- â˜‘ Complements existing practice
- â˜‘ Worth the cost

---

## FEEDBACK REVIEW PROCESS

### Daily Review (During Active Testing)

**Your Role:**
1. Check dashboard for flagged assessments
2. Review 3-5 random assessments per level
3. Note any patterns or concerns
4. Respond to teacher emails promptly
5. Log all issues

### Weekly Review

**With Testing Teachers:**
1. Collect feedback forms
2. Discuss successes and problems
3. Review data trends
4. Make immediate adjustments if needed
5. Plan next week

### End-of-Phase Review

**After Each Testing Phase:**
1. Comprehensive data analysis
2. Teacher focus group/interviews
3. Pupil feedback compilation
4. Issue resolution review
5. Decision: proceed/adjust/pause
6. Update documentation

---

## PILOT COMPLETION DELIVERABLES

### Final Report Contents

**1. Executive Summary**
- Beta test overview
- Key findings
- Recommendations
- Go/No-Go decision

**2. Technical Performance**
- Platform metrics
- Uptime statistics
- Load times
- Error logs
- Device compatibility

**3. AI Assessment Analysis**
- Accuracy rates per level
- Agreement with teachers
- Feedback quality assessment
- Error pattern detection performance
- Cost analysis

**4. Pedagogical Effectiveness**
- Pupil progress data
- Pass rates per level
- Improvement metrics
- Writing samples (before/after)
- Teacher observations

**5. User Feedback**
- Teacher satisfaction data
- Pupil engagement metrics
- Specific quotes
- Suggestions for improvement
- Feature requests

**6. Issues and Resolutions**
- All issues logged
- Resolution status
- Outstanding problems
- Lessons learned

**7. Recommendations**
- Changes needed before launch
- Rubric adjustments
- UI/UX improvements
- Documentation updates
- Training requirements

**8. Next Steps**
- Launch timeline
- Rollout plan
- Marketing strategy
- Pricing confirmation
- Support system

---

## GO/NO-GO DECISION CRITERIA

### READY TO LAUNCH if:
âœ… 85%+ AI assessment accuracy
âœ… 80%+ pupil completion rates
âœ… 70%+ teacher satisfaction
âœ… All P1 and P2 issues resolved
âœ… Technical stability confirmed
âœ… Cost model validated
âœ… Evidence of pupil improvement
âœ… Teachers would continue using

### NEEDS MORE WORK if:
âš ï¸ <80% AI accuracy
âš ï¸ <60% completion rates
âš ï¸ <60% teacher satisfaction
âš ï¸ Outstanding critical issues
âš ï¸ Technical instability
âš ï¸ Cost concerns
âš ï¸ No clear learning benefit

### PIVOT/RECONSIDER if:
ðŸš« <70% AI accuracy
ðŸš« <50% completion rates
ðŸš« Teachers wouldn't use
ðŸš« Fundamental platform problems
ðŸš« Cannot fix within budget
ðŸš« Pupils not improving

---

**BETA TESTING GUIDE COMPLETE: Ready to implement systematic pilot testing!**
